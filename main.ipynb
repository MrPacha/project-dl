{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9424c9-66e4-44d0-bd55-e212d1686332",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Galaxy generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e10d21-7ca1-46f5-a176-3d5a923cae82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9d722-3953-44f8-832b-87d93680d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from utils.custom_utils import RetrieveData, show_images\n",
    "import random as rd\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e0a1f-055b-4575-a53a-2550e20a6933",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657feb4c-3524-4a30-a377-cd1f2587270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataURL='https://tinyurl.com/mr2yc5nx'\n",
    "\n",
    "RetrieveData(DataURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ed245-837a-4a66-9547-dab4691b440d",
   "metadata": {},
   "source": [
    "## dataloader and data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039897b5-4ead-4b0a-843d-ff7424ba0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='./data/images_gz2/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d429ef2-6a16-44f9-be83-3d91a59f53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyZoo2(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, train=None):\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        images = glob(os.path.join(img_dir,'*.jpg'))\n",
    "        rd.shuffle(images)\n",
    "        \n",
    "        cut = int(0.6*len(images))\n",
    "        if train==True:\n",
    "            self.images = images[:cut]\n",
    "        elif train==False:\n",
    "            self.images = images[cut:]\n",
    "        else:\n",
    "            self.images = images\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images[index]\n",
    "        with Image.open(img_path) as im:\n",
    "            if self.transform:\n",
    "                im = self.transform(im)\n",
    "            return im.float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c740e-adfb-465c-bc28-dfce81cdc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=GalaxyZoo2(image_path, transform=transforms.PILToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df82e9-5b5f-4a61-8c20-85a906e25b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943259b4-3a2b-444f-811d-ecf486c1f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=next(iter(trainloader))\n",
    "print(images.shape)\n",
    "show_images(utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fbce8-285c-497f-8fc8-ac765c580870",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5cbe2-2cd9-446b-8d22-5d34a14772b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'this instance use {device}')\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "\n",
    "image_size = 424\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "\n",
    "#hyperparameters of the NN specifically\n",
    "input_features = channels*image_size*image_size # RGB pixel (dim=3)* 424*424 image\n",
    "output_features = 128 # size of output of encoder = input of decoder\n",
    "hidden_features = 200 # arbitrary\n",
    "latent_space_size = 20 #size of the latent space --> arbitrary\n",
    "learning_rate = 3e-4 # arbitrary\n",
    "num_epochs = 10 # arbitrary\n",
    "\n",
    "model_name = \"MLP_VAE\"\n",
    "#directory = \"\"\n",
    "#checkpoint = f\"{directory}/{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a74d-773b-4f2d-a159-95d22498229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=128, latent_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(hidden_dim, output_dim),\n",
    "                                    )\n",
    "        \n",
    "        self.mu = nn.Linear(output_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(output_dim, latent_dim)\n",
    "        \n",
    "        self.latent_mapping = nn.Linear(latent_dim, output_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(output_dim, hidden_dim),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(hidden_dim, input_dim),\n",
    "                                    )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        encoder = self.encoder(x)\n",
    "        mu, logvar = self.mu(encoder), self.logvar(encoder)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def sample_z(self, mu, logvar):\n",
    "        eps = torch.rand_like(mu)\n",
    "        return mu + eps * torch.exp(0.5 * logvar)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        latent_z = self.latent_mapping(z)\n",
    "        out = self.decoder(latent_z)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "        output = self.decode(z)\n",
    "        return output, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7310f1-482c-47b2-8a06-a59aa34f83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElboLoss(x, x_reconstructed, mu, logvar):\n",
    "    criterion = F.binary_cross_entropy(x_reconstructed, x)\n",
    "    KL_div = 0.5*torch.sum(1 + logvar - mu**2 - torch.exp(logvar))\n",
    "    elbo = torch.mean(criterion - KL_div)\n",
    "    return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24bf1d-17c7-445c-a1ac-05ea6632cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = GalaxyZoo2(image_path, transform=transforms.Compose([transforms.PILToTensor(),]), train=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006c331-ff9d-4004-ac54-f79f1fd23a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_VAE(input_dim=input_features).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8b04e-90a6-4c8c-a775-9f9ec73da0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    \"\"\"\n",
    "    Train the model (VAE) onto the data in the trainloader\n",
    "    \"\"\"\n",
    "    train_avg_loss = []\n",
    "    loop = tqdm(enumerate(trainloader))\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_losses = []\n",
    "        loop\n",
    "        \n",
    "        for i,x in loop:\n",
    "            x = x.to(device).view(x.shape[0],-1)\n",
    "            x_reconstructed, mu, logvar = model(x)\n",
    "        \n",
    "            loss = ElboLoss(x, x_reconstructed, mu, logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            train_losses.append(loss.detach())\n",
    "                    \n",
    "        train_avg_loss.append(torch.mean(torch.FloatTensor(train_losses)))\n",
    "\n",
    "    return train_avg_loss #we could also return the reconstruction loss and the regularization loss individually, but in the end it is the ELBO loss that is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a293e-0d42-499f-839d-a3d3190b8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=next(iter(trainloader))\n",
    "test,mu,logvar=model(images.view(images.shape[0],-1))\n",
    "show_images(utils.make_grid(test.view(test.shape[0],3,424,424)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepLearning]",
   "language": "python",
   "name": "conda-env-DeepLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
